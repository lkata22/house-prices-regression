# Kaggle-ის კონკურსის მოკლე მიმოხილვა

ეს პროექტი ეფუძნება Kaggle-ის კონკურსს "House Prices - Advanced Regression Techniques".
მონაცემთა ნაკრები მოიცავს სახლების სხვადასხვა მახასიათებლებს,
როგორებიცაა ფართობი, წლოვანება, სართულების რაოდენობა, რემონტის სტატუსი და ა.შ.

# ჩემი მიდგომა პრობლემის გადასაჭრელად

## პროექტი შედგება რამდენიმე ეტაპისგან:

Cleaning (მონაცემთა გაწმენდა)

Feature Engineering (ახალი ცვლადების შექმნა)

Feature Selection (მნიშვნელოვანი მახასიათებლების შერჩევა)

Training (მოდელის სწავლება)

Hyperparameter Tuning (ჰიპერპარამეტრების ოპტიმიზაცია)

MLflow-სა და DagsHub-ზე ხდებოდა ყველა ექსპერიმენტის ლოგირება.

# რეპოზიტორიის სტრუქტურა

model_experiment.ipynb - მონაცემთა დამუშავების, მოდელის სწავლებისა და ექსპერიმენტების ლოგირების კოდი.

model_inference.ipynb - საუკეთესო მოდელის გამოყენებით პროგნოზის გაკეთება test მონაცემებზე და submission-ის შექმნა.

README.md - პროექტის დეტალური აღწერა.

## Feature Engineering

გამოვიყენე შემდეგი მიდგომები:

TotalSF – სარდაფის, პირველი და მეორე სართულის ფართობების ჯამი.

TotalBath – ყველა აბაზანის რაოდენობა ჯამურად.

TotalPorch – ყველა აივნის ფართობის ჯამი.

Age – სახლის წლოვანება გაყიდვის მომენტისთვის.

RemodAge – რემონტიდან გასული დრო.

IsRemod – ინდიკატორი, ჰქონდა თუ არა სახლს რემონტი.

IsNew – ინდიკატორი, ახალი აშენებული სახლი იყო თუ არა.

## კატეგორიული ცვლადების რიცხვითში გადაყვანა

გამოყენებული იყო OneHotEncoder, რომელიც კატეგორიულ ცვლადებს გარდაქმნის რიცხვით მონაცემებად.

## Nan მნიშვნელობების დამუშავება

რიცხვითი მნიშვნელობები შევავსე მედიანით ან 0-ით, მახასიათებლის მიხედვით.

კატეგორიული მნიშვნელობები შევავსე „None”-ით.

# Cleaning მიდგომები

შექმნილია კლასი HandleMissingValues, რომელიც სისტემატურად ავსებს მონაცემებში არსებულ სიცარიელეებს.

OutlierHandler კლასი ზღუდავს ექსტრემალურ მნიშვნელობებს z-Score-ის გამოყენებით.

# Feature Selection

გამოყენებული იყო ავტომატური feature selection GridSearchCV-ისა და მოდელების გამოყენებით.

# Training

## ტესტირებული მოდელები

LinearRegression

RandomForestRegressor

XGBRegressor

## Hyperparameter ოპტიმიზაციის მიდგომა

GridSearchCV-ით მოხდა ჰიპერპარამეტრების ოპტიმიზაცია cross-validation-ის დახმარებით.

## საბოლოო მოდელის შერჩევის დასაბუთება

საბოლოო მოდელი შეირჩა validation RMSE-ის მიხედვით. მოდელი, რომელმაც ყველაზე დაბალი RMSE აჩვენა, გახდა საბოლოო არჩევანი.

## MLflow Tracking

MLflow-ს გამოყენებით:

დალოგილია ყველა ექსპერიმენტი, მოდელის ჰიპერპარამეტრები და მიღებული მეტრიკები (Train RMSE, Validation RMSE).

საუკეთესო მოდელი შენახულია Model Registry-ში.

# საუკეთესო მოდელის შედეგები

არჩეული მოდელის RMSE მნიშვნელობა ვალიდაციის მონაცემებზე : 0.13114

არჩეული მოდელი : XGBoost

# გამოყენებული ბიბლიოთეკები

numpy, pandas: მონაცემთა მანიპულაცია და ანალიზი.

matplotlib, seaborn: მონაცემთა ვიზუალიზაცია.

scikit-learn: preprocessing, მოდელირება, ჰიპერპარამეტრების ოპტიმიზაცია.

XGBoost: Gradient boosting მოდელი.

MLflow, DagsHub: ექსპერიმენტების და შედეგების დალოგვა.


MLflow link: https://dagshub.com/lkata22/house-prices-regression.mlflow
